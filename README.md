# DID Framework

### ğŸš€ Just Another Ducking ELT Framework ğŸ¦†

---

**DID** is a lightweight, metadata-driven Extract-Load-Transform (ELT) framework designed for simplicity and efficiency.

- ğŸ’» **No terminals, no SQL editors**: Everything happens within your favorite Python environment (e.g., Jupyter notebooks).
- ğŸ“‚ **Metadata-driven ELT**: Easily manage your pipeline using YAML configurations and dbt manifests.
- âš¡ **Powered by DuckDB, Intake, and dbt**: Three open-source tools combined to deliver blazing-fast ELT workflows.

DID makes ELT approachable for analysts, engineers, and data enthusiasts alike. It's designed to just get ELT done.

---

## ğŸŒŸ Key Features

- **Seamless Metadata Integration**:

  - Use YAML files to define input catalogs and dbt manifests.
  - Save and reuse output catalogs for consistent workflows.

- **Python-Powered Simplicity**:

  - No need for external toolsâ€”write, run, and manage ELT pipelines directly in Python.

- **Blazing Performance**:

  - Leverage DuckDB for fast, in-memory data transformations.

---

## ğŸ› ï¸ Example Workflow

With DID, you can execute an entire ELT workflow provided an intake catalog with your sources, and a dbt project in just a few lines of Python:

```
# Run it all with one method:
DID.do(
    input_cat_path='input_cat_file.yml',
    manifest_path=dbt_manifest_path,
    output_cat_path='output_cat_file.yml'
)
```

---

## ğŸ¤” Why DID?

### âœ¨ Simplicity

- Focus on building pipelinesâ€”not wrestling with infrastructure.
- No need to learn additional tools or query languages.

### ğŸ”„ Flexibility

- **Integrate easily**: Compatible with existing dbt projects and YAML-based catalogs.
- **Modular design**: Pick and choose components to fit your use case.

### ğŸ“ˆ Scalability

- From small datasets to enterprise-scale workloads, DID grows with you.

---

## ğŸ“¥ Installation

DID is Python-based and can be installed directly via pip:

```bash
pip install did-framework
```

---

## ğŸš€ Getting Started

1. Define your **Input Catalog**:

   - Create a YAML file (`input_cat_file.yml`) describing your raw data sources.

2. Load your **dbt Manifest**:

   - Provide the path to the `manifest.json` generated by your dbt project.

3. Save your **Output Catalog**:

   - Use DID to generate a new YAML file with transformed data.

4. Automate with the `do` method:

   - Combine extraction, loading, and transformation in one step.

---

## ğŸ¤ Contributing

Weâ€™re excited to build DID into the most developer-friendly ELT framework out there. Contributions are welcome! Feel free to submit issues, feature requests, or pull requests.

---

## ğŸ“§ Contact

Have questions or feedback? Reach out to us on GitHub or at [**did-support@eltducks.com**](mailto\:did-support@eltducks.com).

Letâ€™s get ELT doneâ€”the ducking way!

# underneath the hood 
```
python
# Extracts and Loads

from did.do import extractload, transform

## did.do.extractload
# Extract and Load input catalog with raw sources
input_cat = intake.from_yaml_file('input_cat_file.yml')
el_cat = input_cat.CatalogToDuck.read()

## did.do.transform
# Transform raw sources, export materialized tables and views as an intake catalog
manifest_data = dbtManifest(url=dbt_manifest_path, metadata={"description": "dbt manifest file"})
output_cat = dbtManifestReader(manifest_data).dbtManifestToCatalog.read()

# Save
output_cat.save('output_cat_file.yml')
```




